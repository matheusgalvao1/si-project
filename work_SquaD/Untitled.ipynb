{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e0fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41230f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4703687",
   "metadata": {},
   "source": [
    "!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers==2.5.1\n",
    "!pip install wikipedia==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d894bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_squad(version=1):\n",
    "    if version == 1:\n",
    "        url = [\n",
    "            \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\",\n",
    "            \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "        ]\n",
    "        filename = [\"train-v1.1.json\", \"dev-v1.1.json\"]\n",
    "    else:\n",
    "        url = [\n",
    "            \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\",\n",
    "            \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "        ]\n",
    "        filename = [\"train-v2.0.json\", \"dev-v2.0.json\"]\n",
    "\n",
    "    urllib.request.urlretrieve(url[0], filename[0])\n",
    "    urllib.request.urlretrieve(url[1], filename[1])\n",
    "\n",
    "download_squad(version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbffcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 19 35374   19  6855    0     0  20982      0  0:00:01 --:--:--  0:00:01 21027\n",
      "100 35374  100 35374    0     0   102k      0 --:--:-- --:--:-- --:--:--  103k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -O https://raw.githubusercontent.com/huggingface/transformers/b90745c5901809faef3136ed09a689e7d733526c/examples/run_squad.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4374ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path bert-base-uncased  \\\n",
    "    --output_dir models/bert/ \\\n",
    "    --data_dir data/squad   \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache \\\n",
    "    --do_train  \\\n",
    "    --train_file train-v2.0.json   \\\n",
    "    --version_2_with_negative \\\n",
    "    --do_lower_case  \\\n",
    "    --do_eval   \\\n",
    "    --predict_file dev-v2.0.json   \\\n",
    "    --per_gpu_train_batch_size 2   \\\n",
    "    --learning_rate 3e-5   \\\n",
    "    --num_train_epochs 2.0   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128   \\\n",
    "    --threads 10   \\\n",
    "    --save_steps 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a71b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luanc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-29 23:41:04.486466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-11-29 23:41:04.486519: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\luanc\\OneDrive\\Área de Trabalho\\Faculdade\\11° Mestrado IPB\\Sistemas_Inteligentes\\work1\\run_squad.py\", line 75, in <module>\n",
      "    ALL_MODELS = sum(\n",
      "  File \"C:\\Users\\luanc\\OneDrive\\Área de Trabalho\\Faculdade\\11° Mestrado IPB\\Sistemas_Inteligentes\\work1\\run_squad.py\", line 77, in <genexpr>\n",
      "    tuple(conf.pretrained_config_archive_map.keys())\n",
      "AttributeError: type object 'BertConfig' has no attribute 'pretrained_config_archive_map'\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/bert/bbu_squad2'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForQuestionAnswering\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/bert/bbu_squad2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/bert/bbu_squad2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:560\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    562\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:412\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;124;03mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    411\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 412\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    427\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[0;32m    112\u001b[0m ):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:166\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m     )\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/bert/bbu_squad2'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/bert/bbu_squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./models/bert/bbu_squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b3e4471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.5.1\n",
      "  Downloading transformers-2.5.1-py3-none-any.whl (499 kB)\n",
      "     ------------------------------------ 499.4/499.4 kB 489.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==2.5.1) (3.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\luanc\\appdata\\roaming\\python\\python310\\site-packages (from transformers==2.5.1) (1.23.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==2.5.1) (4.64.0)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==2.5.1) (2022.4.24)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.19-py3-none-any.whl (132 kB)\n",
      "     ------------------------------------ 132.6/132.6 kB 373.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==2.5.1) (2.27.1)\n",
      "Collecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2.tar.gz (64 kB)\n",
      "     -------------------------------------- 64.6/64.6 kB 388.4 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers==2.5.1) (0.4.4)\n",
      "Collecting botocore<1.30.0,>=1.29.19\n",
      "  Downloading botocore-1.29.19-py3-none-any.whl (10.1 MB)\n",
      "     -------------------------------------- 10.1/10.1 MB 531.5 kB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.6/79.6 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==2.5.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==2.5.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==2.5.1) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==2.5.1) (2.0.12)\n",
      "Requirement already satisfied: joblib in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers==2.5.1) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers==2.5.1) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers==2.5.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.19->boto3->transformers==2.5.1) (2.8.2)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yopengl-accelerate (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yopengl-accelerate (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [46 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "  copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "  copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "  copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "  copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "  copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "  copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "  copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "  copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "  copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "  copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "  copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "WARNING: Ignoring invalid distribution -yopengl-accelerate (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yopengl-accelerate (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yopengl-accelerate (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\luanc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19055d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luanc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# executing these commands for the first time initiates a download of the \n",
    "# model weights to ~/.cache/torch/transformers/\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\") \n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6556ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Argead dynasty'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who ruled Macedonia\"\n",
    "\n",
    "context = \"\"\"Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece, \n",
    "and later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled \n",
    "by the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient \n",
    "Macedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th \n",
    "century BC, it was a small kingdom outside of the area dominated by the city-states of Athens, \n",
    "Sparta and Thebes, and briefly subordinate to Achaemenid Persia.\"\"\"\n",
    "\n",
    "\n",
    "# 1. TOKENIZE THE INPUT\n",
    "# note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for \n",
    "# exploration but you cannot feed that into a model. \n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\") \n",
    "\n",
    "# 2. OBTAIN MODEL SCORES\n",
    "# the AutoModelForQuestionAnswering class includes a span predictor on top of the model. \n",
    "# the model returns answer start and end scores for each word in the text\n",
    "r = model(**inputs)\n",
    "\n",
    "\n",
    "\n",
    "answer_start_scores = r['start_logits']\n",
    "answer_end_scores = r['end_logits']\n",
    "answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "\n",
    "# 3. GET THE ANSWER SPAN\n",
    "# once we have the most likely start and end tokens, we grab all the tokens between them\n",
    "# and convert tokens back to words!\n",
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d6133d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia search results for our question:\n",
      "\n",
      "['Wandering albatross',\n",
      " 'Pelagornis sandersi',\n",
      " 'List of largest birds',\n",
      " 'Argentavis',\n",
      " 'Black-browed albatross',\n",
      " 'List of birds by flight speed',\n",
      " 'Largest body part',\n",
      " 'Mollymawk',\n",
      " 'Phosphatodraco',\n",
      " 'List of birds of the Chatham Islands']\n",
      "\n",
      "The Wandering albatross Wikipedia article contains 10427 characters.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wiki\n",
    "import pprint as pp\n",
    "\n",
    "question = 'What is the wingspan of an albatross?'\n",
    "\n",
    "results = wiki.search(question)\n",
    "print(\"Wikipedia search results for our question:\\n\")\n",
    "pp.pprint(results)\n",
    "\n",
    "page = wiki.page(results[0])\n",
    "text = page.content\n",
    "print(f\"\\nThe {results[0]} Wikipedia article contains {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a4edc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This translates into 2536 tokens.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(question, text, return_tensors='pt')\n",
    "print(f\"This translates into {len(inputs['input_ids'][0])} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d9ec1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question consists of 12 tokens.\n",
      "Each chunk will contain 497 tokens of the Wikipedia article.\n"
     ]
    }
   ],
   "source": [
    "# time to chunk!\n",
    "from collections import OrderedDict\n",
    "\n",
    "# identify question tokens (token_type_ids = 0)\n",
    "qmask = inputs['token_type_ids'].lt(1)\n",
    "qt = torch.masked_select(inputs['input_ids'], qmask)\n",
    "print(f\"The question consists of {qt.size()[0]} tokens.\")\n",
    "\n",
    "chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "# having to add a [SEP] token to the end of each chunk\n",
    "print(f\"Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.\")\n",
    "\n",
    "# create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "chunked_input = OrderedDict()\n",
    "for k,v in inputs.items():\n",
    "    q = torch.masked_select(v, qmask)\n",
    "    c = torch.masked_select(v, ~qmask)\n",
    "    chunks = torch.split(c, chunk_size)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i not in chunked_input:\n",
    "            chunked_input[i] = {}\n",
    "\n",
    "        thing = torch.cat((q, chunk))\n",
    "        if i != len(chunks)-1:\n",
    "            if k == 'input_ids':\n",
    "                thing = torch.cat((thing, torch.tensor([102])))\n",
    "            else:\n",
    "                thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91640b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in chunk 0: 512\n",
      "Number of tokens in chunk 1: 512\n",
      "Number of tokens in chunk 2: 512\n",
      "Number of tokens in chunk 3: 512\n",
      "Number of tokens in chunk 4: 512\n",
      "Number of tokens in chunk 5: 41\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunked_input.keys())):\n",
    "    print(f\"Number of tokens in chunk {i}: {len(chunked_input[i]['input_ids'].tolist()[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda1533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_ids_to_string(tokenizer, input_ids):\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "\n",
    "answer = ''\n",
    "\n",
    "# now we iterate over our chunks, looking for the best answer from each chunk\n",
    "for _, chunk in chunked_input.items():\n",
    "    \n",
    "    \n",
    "    r = model(**chunk)\n",
    "\n",
    "    answer_start_scores = r['start_logits']\n",
    "    answer_end_scores = r['end_logits']\n",
    "    ##answer_start_scores, answer_end_scores = model(**chunk)\n",
    "\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    ans = convert_ids_to_string(tokenizer, chunk['input_ids'][0][answer_start:answer_end])\n",
    "    \n",
    "    # if the ans == [CLS] then the model did not find a real answer in this chunk\n",
    "    if ans != '[CLS]':\n",
    "        answer += ans + \" / \"\n",
    "        \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ca2bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):\n",
    "        self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        \"\"\" \n",
    "        Break up a long article into chunks that fit within the max token\n",
    "        requirement for that Transformer model. \n",
    "\n",
    "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
    "        [CLS] question tokens [SEP] context tokens [SEP].\n",
    "        \"\"\"\n",
    "\n",
    "        # create question mask based on token_type_ids\n",
    "        # value is 0 for question tokens, 1 for context tokens\n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "        # having to add an ending [SEP] token to the end\n",
    "\n",
    "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    "\n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                \n",
    "                r = self.model(**chunk)\n",
    "\n",
    "                answer_start_scores = r['start_logits']\n",
    "                answer_end_scores = r['end_logits']\n",
    "                #answer_start_scores, answer_end_scores = self.model(**chunk)\n",
    "\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \" / \"\n",
    "            return answer\n",
    "        else:\n",
    "            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n",
    "\n",
    "            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9250a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide \n",
    "\n",
    "# to make the following output more readable I'll turn off the token sequence length warning\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ad81e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was Barack Obama born?\n",
      "Top wiki result: <WikipediaPage 'Family of Barack Obama'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9437 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: January 17, 1964 / circa 1829 / 1895 / 1934 / c. 1940 / [CLS] When was Barack Obama born? [SEP] governor of the Kenyan county of Siaya in 2013. His campaign slogan was \" Obama here, Obama there \" in reference to his half - brother who was serving his second term as the president of the United States. Malik garnered a meager 2, 792 votes, about 140, 000 votes behind the eventual winner. Prior to the 2016 United States presidential election, he stated that he supported Donald Trump, the candidate for the Republican Party. He attended the third presidential debate as one of Trump ' s guests. = = = Auma Obama = = = Barack Obama ' s half - sister, born c. 1960 / \n",
      "\n",
      "Question: Why is the sky blue?\n",
      "Top wiki result: <WikipediaPage 'Diffuse sky radiation'>\n",
      "Answer: Rayleigh scattering / its intrinsic nature, can illuminate under - canopy leaves permitting more efficient total whole - plant photosynthesis / \n",
      "\n",
      "Question: How many sides does a pentagon have?\n",
      "Top wiki result: <WikipediaPage 'The Pentagon'>\n",
      "Answer: five / five / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'When was Barack Obama born?',\n",
    "    'Why is the sky blue?',\n",
    "    'How many sides does a pentagon have?'\n",
    "]\n",
    "\n",
    "reader = DocumentReader(\"deepset/bert-base-cased-squad2\") \n",
    "\n",
    "# if you trained your own model using the training cell earlier, you can access it with this:\n",
    "#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    results = wiki.search(question)\n",
    "\n",
    "    page = wiki.page(results[0])\n",
    "    print(f\"Top wiki result: {page}\")\n",
    "\n",
    "    text = page.content\n",
    "\n",
    "    reader.tokenize(question, text)\n",
    "    print(f\"Answer: {reader.get_answer()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69904307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
